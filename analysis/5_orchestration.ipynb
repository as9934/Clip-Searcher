{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip-Searcher Pipeline & App\n",
    "\n",
    "This notebook runs the complete Clip-Searcher pipeline:\n",
    "1. Scrape news articles using DuckDuckGo\n",
    "2. Process with NLP (entity extraction + coreference)\n",
    "3. Build relationship graph\n",
    "4. Visualize with PyVis or launch Gradio app\n",
    "\n",
    "Data is saved to `data/raw/` between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEARCH_QUERY = \"artificial intelligence\"\n",
    "ARTICLE_COUNT = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from newspaper import Article\n",
    "from typing import List, Optional, Dict, Set, Tuple, Any\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_news(query: str, count: int = 50) -> List[str]:\n",
    "    \"\"\"Search for news articles using DuckDuckGo.\"\"\"\n",
    "    urls = []\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.news(query, max_results=count, backend='auto')\n",
    "            urls = [r['url'] for r in results if r.get('url')]\n",
    "        logger.info(f\"Found {len(urls)} URLs for query: {query}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search failed: {e}\")\n",
    "    return urls\n",
    "\n",
    "def download_article(url: str) -> Optional[dict]:\n",
    "    \"\"\"Download and parse a single article.\"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {'url': url, 'title': article.title, 'text': article.text}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to download {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_articles(urls: List[str]) -> List[dict]:\n",
    "    \"\"\"Download multiple articles.\"\"\"\n",
    "    articles = [a for url in urls if (a := download_article(url)) and a.get('text')]\n",
    "    logger.info(f\"Downloaded {len(articles)}/{len(urls)} articles\")\n",
    "    return articles\n",
    "\n",
    "def save_urls_to_raw(urls: List[str], query: str) -> Path:\n",
    "    \"\"\"Save URLs to parquet.\"\"\"\n",
    "    df = pd.DataFrame({'url': urls, 'query': query})\n",
    "    filepath = RAW_DATA_DIR / 'urls.parquet'\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    logger.info(f\"Saved {len(urls)} URLs to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_articles_to_raw(articles: List[dict]) -> Path:\n",
    "    \"\"\"Save articles to txt files and metadata to parquet.\"\"\"\n",
    "    articles_dir = RAW_DATA_DIR / 'articles'\n",
    "    articles_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    metadata = []\n",
    "    for article in articles:\n",
    "        url_hash = hashlib.md5(article['url'].encode()).hexdigest()[:12]\n",
    "        txt_path = articles_dir / f\"{url_hash}.txt\"\n",
    "        txt_path.write_text(article['text'], encoding='utf-8')\n",
    "        metadata.append({'url': article['url'], 'title': article['title'], 'filename': f\"{url_hash}.txt\"})\n",
    "    \n",
    "    pd.DataFrame(metadata).to_parquet(RAW_DATA_DIR / 'articles_metadata.parquet', index=False)\n",
    "    logger.info(f\"Saved {len(articles)} articles to {articles_dir}\")\n",
    "    return articles_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scraping\n",
    "urls = search_news(SEARCH_QUERY, count=ARTICLE_COUNT)\n",
    "save_urls_to_raw(urls, SEARCH_QUERY)\n",
    "articles = download_articles(urls)\n",
    "save_articles_to_raw(articles)\n",
    "print(f\"Scraped and saved {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: NLP Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import coreferee\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from dataclasses import dataclass\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    label: str\n",
    "    sent_idx: int\n",
    "    start: int\n",
    "    end: int\n",
    "    url: str\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {'name': self.name, 'label': self.label, 'sent_idx': self.sent_idx,\n",
    "                'start': self.start, 'end': self.end, 'urls': self.url}\n",
    "\n",
    "class NLPProcessor:\n",
    "    def __init__(self, model: str = 'en_core_web_lg'):\n",
    "        self.nlp = spacy.load(model)\n",
    "        self.nlp.add_pipe('coreferee')\n",
    "        self.target_labels = {'PERSON', 'ORG'}\n",
    "    \n",
    "    def clean_entity_name(self, text: str) -> str:\n",
    "        return text.replace('\\n', ' ').replace(\"'s\", \"\").strip()\n",
    "    \n",
    "    def extract_entities(self, sentence: str, sent_idx: int, url: str) -> List[Entity]:\n",
    "        doc = self.nlp(sentence)\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in self.target_labels:\n",
    "                name = self.clean_entity_name(ent.text)\n",
    "                if name:\n",
    "                    entities.append(Entity(name, ent.label_, sent_idx, ent.start, ent.end, url))\n",
    "        return entities\n",
    "    \n",
    "    def get_coreference_chains(self, doc) -> List[List[int]]:\n",
    "        chains = []\n",
    "        if doc._.coref_chains:\n",
    "            for chain in doc._.coref_chains:\n",
    "                indices = []\n",
    "                for mention in chain:\n",
    "                    indices.extend(mention.token_indexes)\n",
    "                chains.append(indices)\n",
    "        return chains\n",
    "    \n",
    "    def get_sentence_for_token(self, doc, token_idx: int) -> int:\n",
    "        for sent_idx, sent in enumerate(doc.sents):\n",
    "            if sent.start <= token_idx < sent.end:\n",
    "                return sent_idx\n",
    "        return -1\n",
    "\n",
    "def load_articles_from_raw() -> List[dict]:\n",
    "    \"\"\"Load articles from data/raw.\"\"\"\n",
    "    metadata_path = RAW_DATA_DIR / 'articles_metadata.parquet'\n",
    "    articles_dir = RAW_DATA_DIR / 'articles'\n",
    "    \n",
    "    if not metadata_path.exists():\n",
    "        return []\n",
    "    \n",
    "    meta_df = pd.read_parquet(metadata_path)\n",
    "    articles = []\n",
    "    for _, row in meta_df.iterrows():\n",
    "        txt_path = articles_dir / row['filename']\n",
    "        if txt_path.exists():\n",
    "            articles.append({'url': row['url'], 'title': row['title'], 'text': txt_path.read_text(encoding='utf-8')})\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NLP processing\n",
    "articles = load_articles_from_raw()\n",
    "processor = NLPProcessor()\n",
    "\n",
    "all_entities = []\n",
    "all_coref_chains = []\n",
    "\n",
    "for article in articles:\n",
    "    if not article.get('text'):\n",
    "        continue\n",
    "    \n",
    "    url = article['url']\n",
    "    text = article['text']\n",
    "    doc = processor.nlp(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sent_idx, sentence in enumerate(sentences):\n",
    "        for ent in processor.extract_entities(sentence, sent_idx, url):\n",
    "            all_entities.append(ent.to_dict())\n",
    "    \n",
    "    chains = processor.get_coreference_chains(doc)\n",
    "    for chain_idx, chain in enumerate(chains):\n",
    "        chain_sents = set()\n",
    "        for token_idx in chain:\n",
    "            sent_idx = processor.get_sentence_for_token(doc, token_idx)\n",
    "            if sent_idx >= 0:\n",
    "                chain_sents.add(sent_idx)\n",
    "        if len(chain_sents) > 1:\n",
    "            all_coref_chains.append({'chain_idx': chain_idx, 'sentences': list(chain_sents), 'url': url})\n",
    "\n",
    "print(f\"Extracted {len(all_entities)} entities and {len(all_coref_chains)} coreference chains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "@dataclass\n",
    "class Edge:\n",
    "    source: str\n",
    "    target: str\n",
    "    edge_type: str\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {'source': self.source, 'target': self.target, 'type': self.edge_type}\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(frozenset([self.source, self.target]))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Edge):\n",
    "            return False\n",
    "        return frozenset([self.source, self.target]) == frozenset([other.source, other.target])\n",
    "\n",
    "def is_valid_pair(e1: dict, e2: dict) -> bool:\n",
    "    if e1['name'] == e2['name'] or not e1['name'] or not e2['name']:\n",
    "        return False\n",
    "    return not (e1['label'] == 'ORG' and e2['label'] == 'ORG')\n",
    "\n",
    "def extract_edges_from_entities(entities: List[dict], coref_chains: List[dict]) -> Set[Edge]:\n",
    "    edges = set()\n",
    "    by_sentence: Dict[Tuple[str, int], List[dict]] = {}\n",
    "    for ent in entities:\n",
    "        key = (ent['urls'], ent['sent_idx'])\n",
    "        by_sentence.setdefault(key, []).append(ent)\n",
    "    \n",
    "    # Same-sentence edges\n",
    "    for ents in by_sentence.values():\n",
    "        if len(ents) >= 2:\n",
    "            for e1, e2 in combinations(ents, 2):\n",
    "                if is_valid_pair(e1, e2):\n",
    "                    edges.add(Edge(e1['name'], e2['name'], f\"{e1['label']}-{e2['label']}\"))\n",
    "    \n",
    "    # Coreference edges\n",
    "    for chain in coref_chains:\n",
    "        chain_entities = []\n",
    "        for sent_idx in chain['sentences']:\n",
    "            key = (chain['url'], sent_idx)\n",
    "            chain_entities.extend(by_sentence.get(key, []))\n",
    "        \n",
    "        if len(chain_entities) >= 2:\n",
    "            for e1, e2 in combinations(chain_entities, 2):\n",
    "                if is_valid_pair(e1, e2):\n",
    "                    edges.add(Edge(e1['name'], e2['name'], f\"{e1['label']}-{e2['label']}\"))\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "edges = extract_edges_from_entities(all_entities, all_coref_chains)\n",
    "edges_df = pd.DataFrame([e.to_dict() for e in edges]) if edges else pd.DataFrame(columns=['source', 'target', 'type'])\n",
    "print(f\"Built graph with {len(edges_df)} edges\")\n",
    "edges_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "ENTITY_COLORS = {\n",
    "    'PERSON': '#4CAF50', 'ORG': '#2196F3',\n",
    "    'PERSON-ORG': '#FF9800', 'ORG-PERSON': '#FF9800', 'PERSON-PERSON': '#9C27B0'\n",
    "}\n",
    "\n",
    "def infer_node_types(edges_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    node_types = {}\n",
    "    for _, row in edges_df.iterrows():\n",
    "        if '-' in str(row.get('type', '')):\n",
    "            t1, t2 = row['type'].split('-')\n",
    "            node_types.setdefault(row['source'], t1)\n",
    "            node_types.setdefault(row['target'], t2)\n",
    "    return node_types\n",
    "\n",
    "def create_visualization(edges_df: pd.DataFrame) -> Network:\n",
    "    G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='type')\n",
    "    node_types = infer_node_types(edges_df)\n",
    "    \n",
    "    net = Network(height='600px', width='100%', bgcolor='#222222', font_color='white',\n",
    "                  notebook=True, cdn_resources='in_line')\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        ntype = node_types.get(node, 'PERSON')\n",
    "        net.add_node(node, label=node, color=ENTITY_COLORS.get(ntype, '#FFFFFF'),\n",
    "                     title=f\"{node} ({ntype})\")\n",
    "    \n",
    "    for s, t, data in G.edges(data=True):\n",
    "        etype = data.get('type', '')\n",
    "        net.add_edge(s, t, color=ENTITY_COLORS.get(etype, '#888888'))\n",
    "    \n",
    "    net.repulsion(node_distance=420, central_gravity=0.33, spring_length=110,\n",
    "                  spring_strength=0.10, damping=0.95)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display visualization\n",
    "if not edges_df.empty:\n",
    "    net = create_visualization(edges_df)\n",
    "    net.show('graph.html')\n",
    "else:\n",
    "    print(\"No edges to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save edges for app\n",
    "edges_df.to_parquet(RAW_DATA_DIR / 'edges.parquet', index=False)\n",
    "print(f\"Saved edges to {RAW_DATA_DIR / 'edges.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio App (Optional)\n",
    "\n",
    "Launch an interactive web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tempfile\n",
    "\n",
    "def create_graph_html(edges_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Create PyVis graph and return HTML content.\"\"\"\n",
    "    if edges_df.empty:\n",
    "        return \"<h3>No relationships found</h3>\"\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='type')\n",
    "    node_types = infer_node_types(edges_df)\n",
    "    \n",
    "    net = Network(height='600px', width='100%', bgcolor='#222222', font_color='white',\n",
    "                  notebook=False, cdn_resources='in_line')\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        ntype = node_types.get(node, 'PERSON')\n",
    "        net.add_node(node, label=node, color=ENTITY_COLORS.get(ntype, '#FFFFFF'),\n",
    "                     title=f\"{node} ({ntype})\")\n",
    "    \n",
    "    for s, t, data in G.edges(data=True):\n",
    "        etype = data.get('type', '')\n",
    "        net.add_edge(s, t, color=ENTITY_COLORS.get(etype, '#888888'))\n",
    "    \n",
    "    net.repulsion(node_distance=420, central_gravity=0.33, spring_length=110,\n",
    "                  spring_strength=0.10, damping=0.95)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.html', delete=False) as f:\n",
    "        net.save_graph(f.name)\n",
    "        return Path(f.name).read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singleton NLP processor for Gradio\n",
    "_nlp_processor = None\n",
    "\n",
    "def get_processor():\n",
    "    global _nlp_processor\n",
    "    if _nlp_processor is None:\n",
    "        _nlp_processor = NLPProcessor()\n",
    "    return _nlp_processor\n",
    "\n",
    "def run_pipeline(query: str, count: int, progress=gr.Progress()) -> str:\n",
    "    \"\"\"Run the full pipeline and return graph HTML.\"\"\"\n",
    "    progress(0.1, desc=\"Searching DuckDuckGo News...\")\n",
    "    urls = search_news(query, count=count)\n",
    "    \n",
    "    progress(0.3, desc=f\"Downloading {len(urls)} articles...\")\n",
    "    articles = download_articles(urls)\n",
    "    \n",
    "    if not articles:\n",
    "        return \"<h3>No articles could be downloaded</h3>\"\n",
    "    \n",
    "    progress(0.5, desc=\"Processing with NLP...\")\n",
    "    proc = get_processor()\n",
    "    all_entities, all_coref_chains = [], []\n",
    "    \n",
    "    for article in articles:\n",
    "        if not article.get('text'):\n",
    "            continue\n",
    "        url, text = article['url'], article['text']\n",
    "        doc = proc.nlp(text)\n",
    "        \n",
    "        for sent_idx, sentence in enumerate(sent_tokenize(text)):\n",
    "            for ent in proc.extract_entities(sentence, sent_idx, url):\n",
    "                all_entities.append(ent.to_dict())\n",
    "        \n",
    "        for chain_idx, chain in enumerate(proc.get_coreference_chains(doc)):\n",
    "            chain_sents = {proc.get_sentence_for_token(doc, t) for t in chain}\n",
    "            chain_sents.discard(-1)\n",
    "            if len(chain_sents) > 1:\n",
    "                all_coref_chains.append({'chain_idx': chain_idx, 'sentences': list(chain_sents), 'url': url})\n",
    "    \n",
    "    progress(0.8, desc=\"Building graph...\")\n",
    "    edges = extract_edges_from_entities(all_entities, all_coref_chains)\n",
    "    edges_df = pd.DataFrame([e.to_dict() for e in edges]) if edges else pd.DataFrame(columns=['source', 'target', 'type'])\n",
    "    edges_df.to_parquet(RAW_DATA_DIR / 'edges.parquet', index=False)\n",
    "    \n",
    "    progress(0.9, desc=\"Creating visualization...\")\n",
    "    return create_graph_html(edges_df)\n",
    "\n",
    "def load_existing_graph() -> str:\n",
    "    \"\"\"Load pre-computed graph from data/raw.\"\"\"\n",
    "    edges_path = RAW_DATA_DIR / 'edges.parquet'\n",
    "    if not edges_path.exists():\n",
    "        return \"<h3>No pre-computed graph found. Run the pipeline first.</h3>\"\n",
    "    return create_graph_html(pd.read_parquet(edges_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio app\n",
    "with gr.Blocks(title=\"clip-search.ai\", theme=gr.themes.Soft()) as app:\n",
    "    gr.Markdown(\"# \ud83d\udd0d clip-search.ai\")\n",
    "    gr.Markdown(\"Entity Relationship Graph Generator from News Articles\")\n",
    "    \n",
    "    with gr.Tab(\"Search & Generate\"):\n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(label=\"Search Query\", placeholder=\"e.g., artificial intelligence\")\n",
    "            count_input = gr.Slider(minimum=10, maximum=100, value=20, step=10, label=\"Number of Articles\")\n",
    "        search_btn = gr.Button(\"\ud83d\ude80 Search and Build Graph\", variant=\"primary\")\n",
    "        graph_output = gr.HTML(label=\"Entity Relationship Graph\")\n",
    "        search_btn.click(fn=run_pipeline, inputs=[query_input, count_input], outputs=graph_output)\n",
    "    \n",
    "    with gr.Tab(\"Load Existing\"):\n",
    "        gr.Markdown(\"Load a previously generated graph from `data/raw/edges.parquet`\")\n",
    "        load_btn = gr.Button(\"\ud83d\udcc2 Load Graph\", variant=\"secondary\")\n",
    "        existing_graph = gr.HTML(label=\"Loaded Graph\")\n",
    "        load_btn.click(fn=load_existing_graph, outputs=existing_graph)\n",
    "\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}