{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Processing Module\n",
    "\n",
    "This notebook contains NLP utilities for entity extraction and coreference resolution using spaCy and Coreferee.\n",
    "Reads article data from `data/raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import coreferee\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Data directories\n",
    "RAW_DATA_DIR = Path('../data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_from_raw() -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load articles from data/raw directory.\n",
    "    \n",
    "    Returns:\n",
    "        List of article dictionaries with 'url' and 'text' keys\n",
    "    \"\"\"\n",
    "    metadata_path = RAW_DATA_DIR / 'articles_metadata.parquet'\n",
    "    articles_dir = RAW_DATA_DIR / 'articles'\n",
    "    \n",
    "    if not metadata_path.exists():\n",
    "        logger.warning(f\"No metadata found at {metadata_path}\")\n",
    "        return []\n",
    "    \n",
    "    meta_df = pd.read_parquet(metadata_path)\n",
    "    articles = []\n",
    "    \n",
    "    for _, row in meta_df.iterrows():\n",
    "        txt_path = articles_dir / row['filename']\n",
    "        if txt_path.exists():\n",
    "            text = txt_path.read_text(encoding='utf-8')\n",
    "            articles.append({\n",
    "                'url': row['url'],\n",
    "                'title': row['title'],\n",
    "                'text': text\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Loaded {len(articles)} articles from {articles_dir}\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"Represents an extracted named entity.\"\"\"\n",
    "    name: str\n",
    "    label: str\n",
    "    sent_idx: int\n",
    "    start: int\n",
    "    end: int\n",
    "    url: str\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'label': self.label,\n",
    "            'sent_idx': self.sent_idx,\n",
    "            'start': self.start,\n",
    "            'end': self.end,\n",
    "            'urls': self.url\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CoreferenceChain:\n",
    "    \"\"\"Represents a coreference chain.\"\"\"\n",
    "    chain_idx: int\n",
    "    mentions: List[Tuple[int, int]]  # (token_start, sent_idx)\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPProcessor:\n",
    "    \"\"\"\n",
    "    NLP processor using spaCy and Coreferee for entity extraction\n",
    "    and coreference resolution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'en_core_web_lg'):\n",
    "        \"\"\"\n",
    "        Initialize NLP processor.\n",
    "        \n",
    "        Args:\n",
    "            model: spaCy model name (use 'lg' or 'trf' for better accuracy)\n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load(model)\n",
    "        self.nlp.add_pipe('coreferee')\n",
    "        self.target_labels = {'PERSON', 'ORG'}\n",
    "    \n",
    "    def clean_entity_name(self, text: str) -> str:\n",
    "        \"\"\"Clean up entity name text.\"\"\"\n",
    "        return text.replace('\\n', ' ').replace(\"'s\", \"\").strip()\n",
    "    \n",
    "    def extract_entities_from_sentence(\n",
    "        self, \n",
    "        sentence: str, \n",
    "        sent_idx: int, \n",
    "        url: str\n",
    "    ) -> List[Entity]:\n",
    "        \"\"\"\n",
    "        Extract PERSON and ORG entities from a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Text of the sentence\n",
    "            sent_idx: Index of the sentence in the document\n",
    "            url: Source URL\n",
    "        \n",
    "        Returns:\n",
    "            List of Entity objects\n",
    "        \"\"\"\n",
    "        doc = self.nlp(sentence)\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in self.target_labels:\n",
    "                name = self.clean_entity_name(ent.text)\n",
    "                if name:  # Skip empty names\n",
    "                    entities.append(Entity(\n",
    "                        name=name,\n",
    "                        label=ent.label_,\n",
    "                        sent_idx=sent_idx,\n",
    "                        start=ent.start,\n",
    "                        end=ent.end,\n",
    "                        url=url\n",
    "                    ))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def get_coreference_chains(self, doc) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Extract coreference chains from a spaCy doc.\n",
    "        \n",
    "        Args:\n",
    "            doc: spaCy Doc object\n",
    "        \n",
    "        Returns:\n",
    "            List of chains, where each chain is a list of token indices\n",
    "        \"\"\"\n",
    "        chains = []\n",
    "        \n",
    "        if doc._.coref_chains:\n",
    "            for chain in doc._.coref_chains:\n",
    "                mention_indices = []\n",
    "                for mention in chain:\n",
    "                    mention_indices.extend(mention.token_indexes)\n",
    "                chains.append(mention_indices)\n",
    "        \n",
    "        return chains\n",
    "    \n",
    "    def get_sentence_index_for_token(self, doc, token_idx: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the sentence index for a given token index.\n",
    "        \n",
    "        Args:\n",
    "            doc: spaCy Doc object\n",
    "            token_idx: Index of the token\n",
    "        \n",
    "        Returns:\n",
    "            Sentence index\n",
    "        \"\"\"\n",
    "        for sent_idx, sent in enumerate(doc.sents):\n",
    "            if sent.start <= token_idx < sent.end:\n",
    "                return sent_idx\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(processor: NLPProcessor, article: dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single article to extract entities and coreferences.\n",
    "    \n",
    "    Args:\n",
    "        processor: NLPProcessor instance\n",
    "        article: Article dictionary with 'url' and 'text' keys\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with entities and coreference data\n",
    "    \"\"\"\n",
    "    url = article['url']\n",
    "    text = article['text']\n",
    "    \n",
    "    # Process full document for coreference\n",
    "    doc = processor.nlp(text)\n",
    "    \n",
    "    # Get sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Extract entities from each sentence\n",
    "    all_entities = []\n",
    "    for sent_idx, sentence in enumerate(sentences):\n",
    "        entities = processor.extract_entities_from_sentence(sentence, sent_idx, url)\n",
    "        all_entities.extend(entities)\n",
    "    \n",
    "    # Get coreference chains\n",
    "    coref_chains = processor.get_coreference_chains(doc)\n",
    "    \n",
    "    # Map tokens to sentences for coreference\n",
    "    coref_sentence_mapping = []\n",
    "    for chain_idx, chain in enumerate(coref_chains):\n",
    "        chain_sentences = set()\n",
    "        for token_idx in chain:\n",
    "            sent_idx = processor.get_sentence_index_for_token(doc, token_idx)\n",
    "            if sent_idx >= 0:\n",
    "                chain_sentences.add(sent_idx)\n",
    "        \n",
    "        if len(chain_sentences) > 1:  # Only interested in cross-sentence chains\n",
    "            coref_sentence_mapping.append({\n",
    "                'chain_idx': chain_idx,\n",
    "                'sentences': list(chain_sentences),\n",
    "                'url': url\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'entities': [e.to_dict() for e in all_entities],\n",
    "        'coreference_chains': coref_sentence_mapping\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(articles: List[dict]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process multiple articles for NLP analysis.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        List of processed article data\n",
    "    \"\"\"\n",
    "    processor = NLPProcessor()\n",
    "    results = []\n",
    "    \n",
    "    for article in articles:\n",
    "        if article.get('text'):\n",
    "            try:\n",
    "                result = process_article(processor, article)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to process article {article.get('url')}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Successfully processed {len(results)}/{len(articles)} articles\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process sample text\n",
    "# processor = NLPProcessor()\n",
    "# sample_article = {\n",
    "#     'url': 'https://example.com/article',\n",
    "#     'text': 'Elon Musk announced that Tesla will expand operations. He said the company plans to hire more engineers.'\n",
    "# }\n",
    "# result = process_article(processor, sample_article)\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
