{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Module\n",
    "\n",
    "This notebook contains utilities for scraping news articles using DuckDuckGo Search and newspaper4k.\n",
    "Scraped data is saved to `data/raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from newspaper import Article\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Data directories\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_news(query: str, count: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for news articles using DuckDuckGo.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        count: Target number of articles to collect\n",
    "    \n",
    "    Returns:\n",
    "        List of news article URLs\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.news(query, max_results=count, backend='auto')\n",
    "            urls = [r['url'] for r in results if r.get('url')]\n",
    "        logger.info(f\"Found {len(urls)} URLs for query: {query}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search failed: {e}\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_article(url: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Download and parse a single article using newspaper4k.\n",
    "    \n",
    "    Args:\n",
    "        url: URL of the article to download\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with article data or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': article.title,\n",
    "            'text': article.text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to download article {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_articles(urls: List[str]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Download multiple articles.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of article URLs\n",
    "    \n",
    "    Returns:\n",
    "        List of article data dictionaries\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    for url in urls:\n",
    "        article_data = download_article(url)\n",
    "        if article_data and article_data.get('text'):\n",
    "            articles.append(article_data)\n",
    "        \n",
    "    logger.info(f\"Successfully downloaded {len(articles)}/{len(urls)} articles\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_urls_to_raw(urls: List[str], query: str) -> Path:\n",
    "    \"\"\"\n",
    "    Save scraped URLs to data/raw as parquet.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of article URLs\n",
    "        query: Search query used\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'url': urls, 'query': query})\n",
    "    filepath = RAW_DATA_DIR / 'urls.parquet'\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    logger.info(f\"Saved {len(urls)} URLs to {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_raw(articles: List[dict]) -> Path:\n",
    "    \"\"\"\n",
    "    Save article texts to data/raw as individual txt files.\n",
    "    Also saves metadata as parquet.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Path to articles directory\n",
    "    \"\"\"\n",
    "    articles_dir = RAW_DATA_DIR / 'articles'\n",
    "    articles_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    metadata = []\n",
    "    for article in articles:\n",
    "        # Create unique filename from URL hash\n",
    "        url_hash = hashlib.md5(article['url'].encode()).hexdigest()[:12]\n",
    "        txt_path = articles_dir / f\"{url_hash}.txt\"\n",
    "        \n",
    "        # Save text\n",
    "        txt_path.write_text(article['text'], encoding='utf-8')\n",
    "        \n",
    "        # Track metadata\n",
    "        metadata.append({\n",
    "            'url': article['url'],\n",
    "            'title': article['title'],\n",
    "            'filename': f\"{url_hash}.txt\"\n",
    "        })\n",
    "    \n",
    "    # Save metadata\n",
    "    meta_df = pd.DataFrame(metadata)\n",
    "    meta_df.to_parquet(RAW_DATA_DIR / 'articles_metadata.parquet', index=False)\n",
    "    \n",
    "    logger.info(f\"Saved {len(articles)} articles to {articles_dir}\")\n",
    "    return articles_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Search for news, download articles, and save to data/raw\n",
    "# urls = search_news(\"artificial intelligence\", count=20)\n",
    "# save_urls_to_raw(urls, \"artificial intelligence\")\n",
    "# articles = download_articles(urls)\n",
    "# save_articles_to_raw(articles)\n",
    "# print(f\"Downloaded and saved {len(articles)} articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
