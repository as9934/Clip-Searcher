{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a9d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from newspaper import Article\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "\n",
    "#get selenium and associated packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "\n",
    "#analysis packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#everything else\n",
    "from itertools import product\n",
    "from more_itertools import unique_everseen\n",
    "import json\n",
    "\n",
    "#spacy stuff\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66422593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py:359: UserWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  warnings.warn(\"find_elements_by_* commands are deprecated. Please use find_elements() instead\")\n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:42: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "/Users/arijitsen/opt/anaconda3/envs/anlp_spacy2/lib/python3.7/site-packages/ipykernel_launcher.py:54: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n"
     ]
    }
   ],
   "source": [
    "graph = {}\n",
    "\n",
    "##Get news articles:\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome('/Users/arijitsen/Downloads/chromedriver')\n",
    "\n",
    "driver.get('http://www.google.com')\n",
    "\n",
    "search_query = driver.find_element_by_name('q')\n",
    "\n",
    "search_query.send_keys('\"Social Sentinel\"')\n",
    "sleep(5)\n",
    "search_query.send_keys(Keys.RETURN)\n",
    "\n",
    "news_urls = []\n",
    "\n",
    "news_section = driver.find_elements_by_class_name('hdtb-mitem')\n",
    "counter = 0\n",
    "for i in news_section:\n",
    "    if counter == 1:\n",
    "        elements = i.find_elements_by_tag_name(\"a\")\n",
    "        for j in elements:\n",
    "            x = j.get_attribute(\"href\")\n",
    "            driver.get(x)\n",
    "        counter += 1\n",
    "    else: \n",
    "        counter += 1\n",
    "\n",
    "content_blocks = driver.find_elements_by_class_name('ftSUBd')\n",
    "\n",
    "for block in content_blocks:\n",
    "    elements = block.find_elements_by_tag_name(\"a\")\n",
    "    for el in elements:\n",
    "        news_urls.append(el.get_attribute(\"href\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    while len(news_urls) < 30:\n",
    "        next_button = driver.find_element_by_id('pnnext')\n",
    "        next_button.click()\n",
    "        news_section = driver.find_elements_by_class_name('hdtb-mitem')\n",
    "        counter = 0\n",
    "        for i in news_section:\n",
    "            if counter == 1:\n",
    "                elements = i.find_elements_by_tag_name(\"a\")\n",
    "                for j in elements:\n",
    "                    x = j.get_attribute(\"href\")\n",
    "                    driver.get(x)\n",
    "                counter += 1\n",
    "            else: \n",
    "                counter += 1\n",
    "\n",
    "        content_blocks = driver.find_elements_by_class_name('ftSUBd')\n",
    "\n",
    "        for block in content_blocks:\n",
    "            elements = block.find_elements_by_tag_name(\"a\")\n",
    "            for el in elements:\n",
    "                news_urls.append(el.get_attribute(\"href\"))\n",
    "        sleep(5)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b177699",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Entity extraction \n",
    "\n",
    "#for each entity we need the entity type (person or org), the sentence index, the word index, the article its in, coreference chain \n",
    "\n",
    "nodes = []\n",
    "\n",
    "urls = []\n",
    "sent_idx = []\n",
    "toke_idx = []\n",
    "\n",
    "corefs = []\n",
    "coref_idx = []\n",
    "urls1 = []\n",
    "for i in range(0, len(news_urls)):\n",
    "    try:\n",
    "        article = Article(news_urls[i])\n",
    "        entities = []\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "\n",
    "\n",
    "        #get the full text of the article\n",
    "        text = article.text\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        tokenizer = nlp.tokenizer \n",
    "\n",
    "        sents = sent_tokenize(doc.text)\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for j,k in enumerate(sents):\n",
    "\n",
    "            x = tokenizer(k) #split sent into words\n",
    "            \n",
    "            sent_idx += len(x) * [j]\n",
    "\n",
    "            for l in x:\n",
    "                counter += 1\n",
    "                toke_idx.append((counter - 1))\n",
    "                urls.append(news_urls[i])\n",
    "     \n",
    "        \n",
    "\n",
    "        \n",
    "        for idx, chain in enumerate(doc._.coref_clusters):\n",
    "            for mention in chain.mentions:\n",
    "                coref_idx.append(idx)\n",
    "                corefs.append(mention.start)\n",
    "                urls1.append(news_urls[i])\n",
    "                \n",
    "\n",
    "\n",
    "        for k, l in enumerate(sents):\n",
    "\n",
    "            #Use Spacy to extract all the entities from the sentence and them add them to the entities list\n",
    "            entity = nlp(l)\n",
    "\n",
    "\n",
    "            for ee in entity.ents:\n",
    "                ent_dict = {}\n",
    "                #we only want person or organization entities\n",
    "                if ee.label_ == 'PERSON':\n",
    "                    ent_dict['name'] = ee.text.replace('\\n',' ').replace(\"’s'\", \"\").strip()\n",
    "                    ent_dict['label'] = ee.label_\n",
    "                    ent_dict['sent_idx'] = k\n",
    "                    ent_dict['start'] = ee.start\n",
    "                    ent_dict['end'] = ee.end\n",
    "                    ent_dict['urls'] = news_urls[i]\n",
    "                    nodes.append(ent_dict)\n",
    "                elif ee.label_ == 'ORG':\n",
    "                    ent_dict['name'] = ee.text.replace('\\n',' ').replace(\"’s\", \"\").strip()\n",
    "                    ent_dict['label'] = ee.label_\n",
    "                    ent_dict['sent_idx'] = k \n",
    "                    ent_dict['start'] = ee.start\n",
    "                    ent_dict['end'] = ee.end\n",
    "                    ent_dict['urls'] = news_urls[i]\n",
    "                    nodes.append(ent_dict)\n",
    "                else:\n",
    "                    pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "d =  {'sent_idx': sent_idx, 'toke_idx':toke_idx, 'urls':urls}\n",
    "e = {'toke_idx':corefs, 'coref_idx': coref_idx, 'urls':urls1}\n",
    "df = pd.DataFrame(d)\n",
    "dfa = pd.DataFrame(e)\n",
    "dfb  = pd.DataFrame(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88e1020c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social Media Security Market</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.taiwannews.com.tw/en/news/4377406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Request</td>\n",
       "      <td>ORG</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.taiwannews.com.tw/en/news/4377406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Download Sample of This</td>\n",
       "      <td>ORG</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.taiwannews.com.tw/en/news/4377406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Information and Communications Technology Market</td>\n",
       "      <td>ORG</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>https://www.taiwannews.com.tw/en/news/4377406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICT</td>\n",
       "      <td>ORG</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.taiwannews.com.tw/en/news/4377406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>Edwards</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/police-tight-lip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>RNZ</td>\n",
       "      <td>ORG</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/police-tight-lip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>Gavaghan</td>\n",
       "      <td>ORG</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/police-tight-lip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>OIA</td>\n",
       "      <td>ORG</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/police-tight-lip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>Data Ethics Advisory</td>\n",
       "      <td>ORG</td>\n",
       "      <td>44</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/police-tight-lip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1410 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name   label  sent_idx  \\\n",
       "0                         Social Media Security Market     ORG         1   \n",
       "1                                              Request     ORG         5   \n",
       "2                              Download Sample of This     ORG         5   \n",
       "3     Information and Communications Technology Market     ORG         6   \n",
       "4                                                  ICT     ORG         6   \n",
       "...                                                ...     ...       ...   \n",
       "1405                                           Edwards  PERSON        38   \n",
       "1406                                               RNZ     ORG        38   \n",
       "1407                                          Gavaghan     ORG        40   \n",
       "1408                                               OIA     ORG        40   \n",
       "1409                              Data Ethics Advisory     ORG        44   \n",
       "\n",
       "      start  end                                               urls  \n",
       "0         0    4      https://www.taiwannews.com.tw/en/news/4377406  \n",
       "1         0    1      https://www.taiwannews.com.tw/en/news/4377406  \n",
       "2         2    6      https://www.taiwannews.com.tw/en/news/4377406  \n",
       "3        13   18      https://www.taiwannews.com.tw/en/news/4377406  \n",
       "4        19   20      https://www.taiwannews.com.tw/en/news/4377406  \n",
       "...     ...  ...                                                ...  \n",
       "1405     29   30  https://www.nzherald.co.nz/nz/police-tight-lip...  \n",
       "1406     31   32  https://www.nzherald.co.nz/nz/police-tight-lip...  \n",
       "1407      0    1  https://www.nzherald.co.nz/nz/police-tight-lip...  \n",
       "1408     24   25  https://www.nzherald.co.nz/nz/police-tight-lip...  \n",
       "1409     18   21  https://www.nzherald.co.nz/nz/police-tight-lip...  \n",
       "\n",
       "[1410 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "03703bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for j in set(dfb['urls']): #filter to a single url\n",
    "    df1a = dfb.loc[dfb['urls'] == j]\n",
    "    for i in set(df1a['sent_idx']): #filter to that sentence\n",
    "        link = {}\n",
    "        df1 = df1a.loc[df1a['sent_idx'] == i]\n",
    "        if len(df1) > 1:\n",
    "            name1 = df1['name']\n",
    "            name2 = df1['name']\n",
    "            x = product(name1, name2)\n",
    "            x = list(x)\n",
    "            x = [k for k in x if k[0] !=  k[1]] #delete links where both nodes are the same\n",
    "            x = list(unique_everseen(x, key=frozenset)) #delete reverse tuples\n",
    "            x = [l for l in x if l[0] != '' and l[1] != ''] #delete links with empty strings\n",
    "            \n",
    "            if len(x) == 0:\n",
    "                pass\n",
    "            elif len(x) == 1:\n",
    "                source = x[0][0]\n",
    "                target = x[0][1]\n",
    "                type1 = df1.loc[df1['name'] == source, 'label'].iloc[0]\n",
    "                type2 = df1.loc[df1['name'] == target, 'label'].iloc[0]\n",
    "                \n",
    "                if type1 == 'ORG' and type2 == 'ORG':\n",
    "                    pass\n",
    "                else:\n",
    "                    link['source'] = source\n",
    "                    link['target'] = target\n",
    "                    link['type'] = f'{type1}-{type2}'\n",
    "                \n",
    "                if link not in links:\n",
    "                        links.append(link)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                for m in range(0, len(x)):\n",
    "                    source = x[m][0]\n",
    "                    target = x[m][1]\n",
    "                    type1 = df1.loc[df1['name'] == source, 'label'].iloc[0]\n",
    "                    type2 = df1.loc[df1['name'] == target, 'label'].iloc[0]\n",
    "                    if type1 == 'ORG' and type2 == 'ORG':\n",
    "                        pass\n",
    "                    else:\n",
    "                        link['source'] = source\n",
    "                        link['target'] = target\n",
    "                        link['type'] = f'{type1}-{type2}'\n",
    "                    \n",
    "                    if link not in links:\n",
    "                        links.append(link)\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a7a393fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, dfa, how='inner', on=['toke_idx', 'urls']) #to get sent index for corefs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "39a1e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(urls):\n",
    "    df2a = df.loc[df['urls'] == i]\n",
    "    df3a = dfb.loc[df['urls'] == i]\n",
    "    #get the sent_idx of every token with the same coref_idx\n",
    "    for j in set(df2a['coref_idx']):\n",
    "        df2 = df2a.loc[df2a['coref_idx'] == j]\n",
    "        if len(set(df2['sent_idx'])) > 1:\n",
    "            link  = {}\n",
    "            coref_sents = list(set(df2['sent_idx']))\n",
    "            df3 = df3a[df3a['sent_idx'].isin(coref_sents)]\n",
    "            name1 = list(df3['name'])\n",
    "            name2 = name1\n",
    "            x = list(product(name1, name2))\n",
    "            x = [k for k in x if k[0] !=  k[1]] #delete links where both nodes are the same\n",
    "            x = list(unique_everseen(x, key=frozenset)) #delete reverse tuples\n",
    "            x = [l for l in x if l[0] != '' and l[1] != ''] #delete links with empty strings\n",
    "            \n",
    "            if len(x) == 0:\n",
    "                pass\n",
    "            \n",
    "            elif len(x) == 1:\n",
    "                source = x[0][0]\n",
    "                target = x[0][1]\n",
    "                type1 = df3.loc[df3['name'] == source, 'label'].iloc[0]\n",
    "                type2 = df3.loc[df3['name'] == target, 'label'].iloc[0]\n",
    "                \n",
    "                if type1 == 'ORG' and type2 == 'ORG':\n",
    "                    pass\n",
    "                else:\n",
    "                    link['type'] = f'{type1}-{type2}'\n",
    "                    link['target'] = x[0][1]\n",
    "                    link['source'] = x[0][0]\n",
    "                \n",
    "                if link not in links:\n",
    "                        links.append(link)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                for m in range(0, len(x)):\n",
    "                    source = x[m][0]\n",
    "                    target = x[m][1]\n",
    "                    type1 = df3.loc[df3['name'] == source, 'label'].iloc[0]\n",
    "                    type2 = df3.loc[df3['name'] == target, 'label'].iloc[0]\n",
    "                    \n",
    "                    if type1 == 'ORG' and type2 == 'ORG':\n",
    "                        pass\n",
    "                    else:\n",
    "                        link['type'] = f'{type1}-{type2}'\n",
    "                        link['source'] = source\n",
    "                        link['target'] = target\n",
    "                        \n",
    "                    \n",
    "                    if link not in links:\n",
    "                        links.append(link)\n",
    "                    else:\n",
    "                        pass\n",
    "        else:\n",
    "            pass\n",
    "links = [link for link in links if link != {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "541730a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph['nodes'] = nodes\n",
    "graph['links'] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c35d6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"graph.json\", \"w\") as outfile:\n",
    "    json.dump(graph, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "59ae0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.DataFrame(links)\n",
    "links_df.to_csv('links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2056cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.to_csv('nodes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
